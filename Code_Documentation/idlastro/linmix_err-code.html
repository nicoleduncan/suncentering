<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<!-- Generated by IDLdoc 3.5.1 on Mon Sep 30 16:57:45 2013 -->

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
    <title>linmix_err.pro (Documentation for ./)</title>

    
    <link rel="stylesheet" type="text/css" media="all"
          href="../idldoc-resources/main.css" />
    <link rel="stylesheet" type="text/css" media="print"
          href="../idldoc-resources/main-print.css" />
    

    <script type="text/javascript">
      function setTitle() {
        parent.document.title="linmix_err.pro (Documentation for ./)";
      }
    </script>
  </head>

  <body onload="setTitle();" id="root">
    <div class="content">
      <code class="source"><span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;+</span>
<span class="comments">;   NAME:</span>
<span class="comments">;     LINMIX_ERR</span>
<span class="comments">;   PURPOSE:</span>
<span class="comments">;      Bayesian approach to linear regression with errors in both X and Y</span>
<span class="comments">;   EXPLANATION:</span>
<span class="comments">;     Perform linear regression of y on x when there are measurement</span>
<span class="comments">;     errors in both variables. the regression assumes :</span>
<span class="comments">;</span>
<span class="comments">;                 ETA = ALPHA + BETA * XI + EPSILON</span>
<span class="comments">;                 X = XI + XERR</span>
<span class="comments">;                 Y = ETA + YERR</span>
<span class="comments">;</span>
<span class="comments">;</span>
<span class="comments">; Here, (ALPHA, BETA) are the regression coefficients, EPSILON is the</span>
<span class="comments">; intrinsic random scatter about the regression, XERR is the</span>
<span class="comments">; measurement error in X, and YERR is the measurement error in</span>
<span class="comments">; Y. EPSILON is assumed to be normally-distributed with mean zero and</span>
<span class="comments">; variance SIGSQR. XERR and YERR are assumed to be</span>
<span class="comments">; normally-distributed with means equal to zero, variances XSIG^2 and</span>
<span class="comments">; YSIG^2, respectively, and covariance XYCOV. The distribution of XI</span>
<span class="comments">; is modelled as a mixture of normals, with group proportions PI,</span>
<span class="comments">; mean MU, and variance TAUSQR. Bayesian inference is employed, and</span>
<span class="comments">; a structure containing random draws from the posterior is</span>
<span class="comments">; returned. Convergence of the MCMC to the posterior is monitored</span>
<span class="comments">; using the potential scale reduction factor (RHAT, Gelman et</span>
<span class="comments">; al.2004). In general, when RHAT &lt; 1.1 then approximate convergence</span>
<span class="comments">; is reached.</span>
<span class="comments">;</span>
<span class="comments">; Simple non-detections on y may also be included.</span>
<span class="comments">;</span>
<span class="comments">; CALLING SEQUENCE:</span>
<span class="comments">;</span>
<span class="comments">;     LINMIX_ERR, X, Y, POST, XSIG=, YSIG=, XYCOV=, DELTA=, NGAUSS=, /SILENT, </span>
<span class="comments">;                /METRO, MINITER= , MAXITER= </span>
<span class="comments">;</span>
<span class="comments">;</span>
<span class="comments">; INPUTS :</span>
<span class="comments">;</span>
<span class="comments">;   X - THE OBSERVED INDEPENDENT VARIABLE. THIS SHOULD BE AN</span>
<span class="comments">;       NX-ELEMENT VECTOR.</span>
<span class="comments">;   Y - THE OBSERVED DEPENDENT VARIABLE. THIS SHOULD BE AN NX-ELEMENT</span>
<span class="comments">;       VECTOR.</span>
<span class="comments">;</span>
<span class="comments">; OPTIONAL INPUTS :</span>
<span class="comments">;</span>
<span class="comments">;   XSIG - THE 1-SIGMA MEASUREMENT ERRORS IN X, AN NX-ELEMENT VECTOR.</span>
<span class="comments">;   YSIG - THE 1-SIGMA MEASUREMENT ERRORS IN Y, AN NX-ELEMENT VECTOR.</span>
<span class="comments">;   XYCOV - THE COVARIANCE BETWEEN THE MEASUREMENT ERRORS IN X AND Y,</span>
<span class="comments">;           AND NX-ELEMENT VECTOR.</span>
<span class="comments">;   DELTA - AN NX-ELEMENT VECTOR INDICATING WHETHER A DATA POINT IS</span>
<span class="comments">;           CENSORED OR NOT. IF DELTA[i] = 1, THEN THE SOURCE IS</span>
<span class="comments">;           DETECTED, ELSE IF DELTA[i] = 0 THE SOURCE IS NOT DETECTED</span>
<span class="comments">;           AND Y[i] SHOULD BE AN UPPER LIMIT ON Y[i]. NOTE THAT IF</span>
<span class="comments">;           THERE ARE CENSORED DATA POINTS, THEN THE</span>
<span class="comments">;           MAXIMUM-LIKELIHOOD ESTIMATE (THETA) IS NOT VALID. THE</span>
<span class="comments">;           DEFAULT IS TO ASSUME ALL DATA POINTS ARE DETECTED, IE,</span>
<span class="comments">;           DELTA = REPLICATE(1, NX).</span>
<span class="comments">;   METRO - IF METRO = 1, THEN THE MARKOV CHAINS WILL BE CREATED USING</span>
<span class="comments">;           THE METROPOLIS-HASTINGS ALGORITHM INSTEAD OF THE GIBBS</span>
<span class="comments">;           SAMPLER. THIS CAN HELP THE CHAINS CONVERGE WHEN THE SAMPLE</span>
<span class="comments">;           SIZE IS SMALL OR IF THE MEASUREMENT ERRORS DOMINATE THE</span>
<span class="comments">;           SCATTER IN X AND Y.</span>
<span class="comments">;   SILENT - SUPPRESS TEXT OUTPUT.</span>
<span class="comments">;   MINITER - MINIMUM NUMBER OF ITERATIONS PERFORMED BY THE GIBBS</span>
<span class="comments">;             SAMPLER OR METROPOLIS-HASTINGS ALGORITHM. IN GENERAL,</span>
<span class="comments">;             MINITER = 5000 SHOULD BE SUFFICIENT FOR CONVERGENCE. THE</span>
<span class="comments">;             DEFAULT IS MINITER = 5000. THE MCMC IS STOPPED AFTER </span>
<span class="comments">;             RHAT &lt; 1.1 FOR ALL PARAMETERS OF INTEREST, AND THE</span>
<span class="comments">;             NUMBER OF ITERATIONS PERFORMED IS GREATER THAN MINITER.</span>
<span class="comments">;   MAXITER - THE MAXIMUM NUMBER OF ITERATIONS PERFORMED BY THE</span>
<span class="comments">;             MCMC. THE DEFAULT IS 1D5. THE MCMC IS STOPPED</span>
<span class="comments">;             AUTOMATICALLY AFTER MAXITER ITERATIONS.</span>
<span class="comments">;   NGAUSS - THE NUMBER OF GAUSSIANS TO USE IN THE MIXTURE</span>
<span class="comments">;            MODELLING. THE DEFAULT IS 3. IF NGAUSS = 1, THEN THE</span>
<span class="comments">;            PRIOR ON (MU, TAUSQR) IS ASSUMED TO BE UNIFORM.</span>
<span class="comments">;</span>
<span class="comments">; OUTPUT :</span>
<span class="comments">;</span>
<span class="comments">;    POST - A STRUCTURE CONTAINING THE RESULTS FROM THE MCMC. EACH</span>
<span class="comments">;           ELEMENT OF POST IS A DRAW FROM THE POSTERIOR DISTRIBUTION</span>
<span class="comments">;           FOR EACH OF THE PARAMETERS.</span>
<span class="comments">;</span>
<span class="comments">;             ALPHA - THE CONSTANT IN THE REGRESSION.</span>
<span class="comments">;             BETA - THE SLOPE OF THE REGRESSION.</span>
<span class="comments">;             SIGSQR - THE VARIANCE OF THE INTRINSIC SCATTER.</span>
<span class="comments">;             PI - THE GAUSSIAN WEIGHTS FOR THE MIXTURE MODEL.</span>
<span class="comments">;             MU - THE GAUSSIAN MEANS FOR THE MIXTURE MODEL.</span>
<span class="comments">;             TAUSQR - THE GAUSSIAN VARIANCES FOR THE MIXTURE MODEL.</span>
<span class="comments">;             MU0 - THE HYPERPARAMETER GIVING THE MEAN VALUE OF THE</span>
<span class="comments">;                   GAUSSIAN PRIOR ON MU. ONLY INCLUDED IF NGAUSS ></span>
<span class="comments">;                   1.</span>
<span class="comments">;             USQR - THE HYPERPARAMETER DESCRIBING FOR THE PRIOR</span>
<span class="comments">;                    VARIANCE OF THE INDIVIDUAL GAUSSIAN CENTROIDS</span>
<span class="comments">;                    ABOUT MU0. ONLY INCLUDED IF NGAUSS > 1.</span>
<span class="comments">;             WSQR - THE HYPERPARAMETER DESCRIBING THE `TYPICAL' SCALE</span>
<span class="comments">;                    FOR THE PRIOR ON (TAUSQR,USQR). ONLY INCLUDED IF</span>
<span class="comments">;                    NGAUSS > 1.</span>
<span class="comments">;             XIMEAN - THE MEAN OF THE DISTRIBUTION FOR THE</span>
<span class="comments">;                      INDEPENDENT VARIABLE, XI.</span>
<span class="comments">;             XISIG - THE STANDARD DEVIATION OF THE DISTRIBUTION FOR</span>
<span class="comments">;                     THE INDEPENDENT VARIABLE, XI.</span>
<span class="comments">;             CORR - THE LINEAR CORRELATION COEFFICIENT BETWEEN THE</span>
<span class="comments">;                    DEPENDENT AND INDEPENDENT VARIABLES, XI AND ETA.</span>
<span class="comments">;</span>
<span class="comments">; CALLED ROUTINES :</span>
<span class="comments">;</span>
<span class="comments">;    RANDOMCHI, MRANDOMN, RANDOMGAM, RANDOMDIR, MULTINOM</span>
<span class="comments">;</span>
<span class="comments">; REFERENCES :</span>
<span class="comments">;</span>
<span class="comments">;   Carroll, R.J., Roeder, K., & Wasserman, L., 1999, Flexible</span>
<span class="comments">;     Parametric Measurement Error Models, Biometrics, 55, 44</span>
<span class="comments">;</span>
<span class="comments">;   Kelly, B.C., 2007, Some Aspects of Measurement Error in</span>
<span class="comments">;     Linear Regression of Astronomical Data, The Astrophysical</span>
<span class="comments">;     Journal, 665, 1489 (arXiv:0705.2774)</span>
<span class="comments">;</span>
<span class="comments">;   Gelman, A., Carlin, J.B., Stern, H.S., & Rubin, D.B., 2004,</span>
<span class="comments">;     Bayesian Data Analysis, Chapman & Hall/CRC</span>
<span class="comments">;</span>
<span class="comments">; REVISION HISTORY</span>
<span class="comments">;</span>
<span class="comments">;     AUTHOR : BRANDON C. KELLY, STEWARD OBS., JULY 2006</span>
<span class="comments">;   - MODIFIED PRIOR ON MU0 TO BE UNIFORM OVER [MIN(X),MAX(X)] AND</span>
<span class="comments">;     PRIOR ON USQR TO BE UNIFORM OVER [0, 1.5 * VARIANCE(X)]. THIS</span>
<span class="comments">;     TENDS TO GIVE BETTER RESULTS WITH FEWER GAUSSIANS. (B.KELLY, MAY</span>
<span class="comments">;     2007)</span>
<span class="comments">;   - FIXED BUG SO THE ITERATION COUNT RESET AFTER THE BURNIN STAGE</span>
<span class="comments">;     WHEN SILENT = 1 (B. KELLY, JUNE 2009)</span>
<span class="comments">;   - FIXED BUG WHEN UPDATING MU VIA THE METROPOLIS-HASTING</span>
<span class="comments">;     UPDATE. PREVIOUS VERSIONS DID NO INDEX MUHAT, SO ONLY MUHAT[0]</span>
<span class="comments">;     WAS USED IN THE PROPOSAL DISTRIBUTION. THANKS TO AMY BENDER FOR</span>
<span class="comments">;     POINTING THIS OUT. (B. KELLY, DEC 2011)</span>
<span class="comments">;-</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to compute the hyperbolic arctangent</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="linmix_atanh:source"></a>function linmix_atanh, x

z = 0.5d * ( alog(1 + x) - alog(1 - x) )

return, z
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to compute a robust estimate for the standard deviation of a</span>
<span class="comments">;data set, based on the inter-quartile range</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="linmix_robsig:source"></a>function linmix_robsig, x

nx = n_elements(x)
                                <span class="comments">;get inter-quartile range of x</span>
sorted = sort(x)
iqr = x[sorted[3 * nx / 4]] - x[sorted[nx / 4]]
sdev = stddev(x, /nan)
sigma = min( [sdev, iqr / 1.34] ) <span class="comments">;use robust estimate for sigma</span>
if sigma eq 0 then sigma = sdev

return, sigma
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to compute the log-likelihood of the data</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="loglik_mixerr:source"></a>function loglik_mixerr, x, y, xvar, yvar, xycov, delta, theta, pi, mu, tausqr, Glabel

alpha = theta[0]
beta = theta[1]
sigsqr = theta[2]

nx = n_elements(x)
ngauss = n_elements(pi)

Sigma11 = dblarr(nx, ngauss)
Sigma12 = dblarr(nx, ngauss)
Sigma22 = dblarr(nx, ngauss)
determ = dblarr(nx, ngauss)

for k = 0, ngauss - 1 do begin

    Sigma11[0,k] = beta^2 * tausqr[k] + sigsqr + yvar
    Sigma12[0,k] = beta * tausqr[k] + xycov
    Sigma22[0,k] = tausqr[k] + xvar

    determ[0, k] = Sigma11[*,k] * Sigma22[*,k] - Sigma12[*,k]^2

endfor

det = where(delta eq 1, ndet, comp=cens, ncomp=ncens) <span class="comments">;any non-detections?</span>

loglik = dblarr(nx)

if ndet gt 0 then begin
                                <span class="comments">;compute contribution to</span>
                                <span class="comments">;log-likelihood from the detected</span>
                                <span class="comments">;sources</span>
    for k = 0, ngauss - 1 do begin
        
        gk = where(Glabel[det] eq k, nk)

        if nk gt 0 then begin

            zsqr = (y[det[gk]] - alpha - beta * mu[k])^2 / Sigma11[det[gk],k] + $
              (x[det[gk]] - mu[k])^2 / Sigma22[det[gk],k] - $
              2d * Sigma12[det[gk],k] * (y[det[gk]] - alpha - beta * mu[k]) * $
              (x[det[gk]] - mu[k]) / (Sigma11[det[gk],k] * Sigma22[det[gk],k])
            
            corrz = Sigma12[det[gk],k] / sqrt( Sigma11[det[gk],k] * Sigma22[det[gk],k] )
            
            loglik[det[gk]] = -0.5d * alog(determ[det[gk],k]) - 0.5 * zsqr / (1d - corrz^2)

        endif

    endfor

endif

if ncens gt 0 then begin
                                <span class="comments">;compute contribution to the</span>
                                <span class="comments">;log-likelihood from the</span>
                                <span class="comments">;non-detections</span>
    for k = 0, ngauss - 1 do begin

        gk = where(Glabel[cens] eq k, nk)

        if nk gt 0 then begin
            
            loglikx = -0.5 * alog(Sigma22[cens[gk],k]) - $
              0.5 * (x[cens[gk]] - mu[k])^2 / Sigma22[cens[gk],k]
            
                                <span class="comments">;conditional mean of y, given x and</span>
                                <span class="comments">;G=k</span>
            cmeany = alpha + beta * mu[k] + Sigma12[cens[gk],k] / Sigma22[cens[gk],k] * $
              (x[cens[gk]] - mu[k])
                                <span class="comments">;conditional variance of y, given x</span>
                                <span class="comments">;and G=k</span>
            cvary = Sigma11[cens[gk],k] - Sigma12[cens[gk],k]^2 / Sigma22[cens[gk],k]

                                <span class="comments">;make sure logliky is finite</span>
            logliky = alog(gauss_pdf( (y[cens[gk]] - cmeany) / sqrt(cvary) )) > (-1d300) 
            
            loglik[cens[gk]] = loglikx + logliky
        
        endif

    endfor
    
endif

loglik = total(loglik)

return, loglik
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to compute the log-prior of the data</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="logprior_mixerr:source"></a>function logprior_mixerr, mu, mu0, tausqr, usqr, wsqr

ngauss = n_elements(mu)

if ngauss gt 1 then begin

    logprior_mu = -0.5 * alog(usqr) - 0.5 * (mu - mu0)^2 / usqr
    logprior_mu = total(logprior_mu)
    
    logprior_tausqr = 0.5 * alog(wsqr) - 1.5 * alog(tausqr) - 0.5 * wsqr / tausqr
    logprior_tausqr = total(logprior_tausqr)

    logprior = logprior_mu + logprior_tausqr

endif else logprior = 0d <span class="comments">;if ngauss = 1 then uniform prior</span>

return, logprior
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to perform the Metropolis update for the scale parameter in</span>
<span class="comments">;the Gibbs sampler</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="linmix_metro_update:source"></a>function linmix_metro_update, logpost_new, logpost_old, seed, log_jrat

lograt = logpost_new - logpost_old

if n_elements(log_jrat) gt 0 then lograt = lograt + log_jrat

accept = 0

if lograt gt 0 then accept = 1 else begin
    
    u = randomu(seed)

    if alog(u) le lograt then accept = 1
    
endelse

return, accept
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to acceptance rates for metropolis-hastings algorithm</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="linmix_metro_results:source"></a>pro linmix_metro_results, arate, ngauss

print, ''
print, 'Metropolis-Hastings Acceptance Rates:'

print, '(ALPHA, BETA) : ' + strtrim(arate[0], 1)
print, 'SIGMA^2 : ' + strtrim(arate[1], 1)
print, ''
for k = 0, ngauss - 1 do begin

    print, 'GAUSSIAN ' + strtrim(k+1,1)
    print, '   MEAN : ' + strtrim(arate[2+k], 1)
    print, '   VARIANCE : ' + strtrim(arate[2+k+ngauss], 1)

endfor

if ngauss gt 1 then begin

    print, ''
    print, 'Mu0 : ' + strtrim(arate[2+2*ngauss], 1)
    print, 'u^2 : ' + strtrim(arate[3+2*ngauss], 1)
    print, 'w^2 : ' + strtrim(arate[4+2*ngauss], 1)

endif

print, ''

return
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;                                                                     ;</span>
<span class="comments">;                             MAIN ROUTINE                            ;</span>
<span class="comments">;                                                                     ;</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="linmix_err:source"></a>pro linmix_err, x, y, post, xsig=xsig, ysig=ysig, xycov=xycov, delta=delta, $
                ngauss=ngauss, metro=metro, silent=silent, miniter=miniter, $
                maxiter=maxiter

if n_params() lt 3 then begin

    print, 'Syntax- LINMIX_ERR, X, Y, POST, XSIG=XSIG, YSIG=YSIG, XYCOV=XYCOV,'
    print, '                    DELTA=DELTA, NGAUSS=NGAUSS, /SILENT, /METRO, '
    print, '                    MINITER=MINITER, MAXITER=MAXITER'
    return

endif

<span class="comments">;check inputs and setup defaults</span>

nx = n_elements(x)
if n_elements(y) ne nx then begin
    print, 'Y and X must have the same size.'
    return
endif

if n_elements(xsig) eq 0 and n_elements(ysig) eq 0 then begin
    print, 'Must supply at least one of XSIG or YSIG.'
    return
endif

if n_elements(xsig) eq 0 then begin
    xsig = dblarr(nx)
    xycov = dblarr(nx)
endif
if n_elements(ysig) eq 0 then begin
    ysig = dblarr(nx)
    xycov = dblarr(nx)
endif
if n_elements(xycov) eq 0 then xycov = dblarr(nx)

if n_elements(xsig) ne nx then begin
    print, 'XSIG and X must have the same size.'
    return
endif
if n_elements(ysig) ne nx then begin
    print, 'YSIG and X must have the same size.'
    return
endif
if n_elements(xycov) ne nx then begin
    print, 'XYCOV and X must have the same size.'
    return
endif

if n_elements(delta) eq 0 then delta = replicate(1, nx)
if n_elements(delta) ne nx then begin
    print, 'DELTA and X must have the same size.'
    return
endif

bad = where(finite(x) eq 0 or finite(y) eq 0 or finite(xsig) eq 0 or $
            finite(ysig) eq 0 or finite(xycov) eq 0, nbad)

if nbad gt 0 then begin
    print, 'Non-finite input detected.'
    return
endif

det = where(delta eq 1, ndet, comp=cens, ncomp=ncens) <span class="comments">;get detected data points</span>

if ncens gt 0 then begin

    cens_noerr = where(ysig[cens] eq 0, ncens_noerr)
    if ncens_noerr gt 0 then begin
        print, 'NON-DETECTIONS FOR Y MUST HAVE NON-ZERO MEASUREMENT ERROR VARIANCE.'
        return
    endif

endif

 <span class="comments">;find data points without measurement error</span>
xnoerr = where(xsig eq 0, nxnoerr, comp=xerr, ncomp=nxerr)
ynoerr = where(ysig eq 0, nynoerr, comp=yerr, ncomp=nyerr)

if nxerr gt 0 then ynoerr2 = where(ysig[xerr] eq 0, nynoerr2) else nynoerr2 = 0L
if nyerr gt 0 then xnoerr2 = where(xsig[yerr] eq 0, nxnoerr2) else nxnoerr2 = 0L

xvar = xsig^2
yvar = ysig^2
xycorr = xycov / (xsig * ysig)
if nxnoerr gt 0 then xycorr[xnoerr] = 0d
if nynoerr gt 0 then xycorr[ynoerr] = 0d

if not keyword_set(metro) then metro = 0
if metro then gibbs = 0 else gibbs = 1
if not keyword_set(silent) then silent = 0
if n_elements(ngauss) eq 0 then ngauss = 3

if ngauss le 0 then begin
    print, 'NGAUSS must be at least 1.'
    return
endif

if n_elements(miniter) eq 0 then miniter = 5000L <span class="comments">;minimum number of iterations that the </span>
                                                 <span class="comments">;Markov Chain must perform</span>
if n_elements(maxiter) eq 0 then maxiter = 100000L <span class="comments">;maximum number of iterations that the </span>
                                                   <span class="comments">;Markov Chain will perform</span>

<span class="comments">;; perform MCMC</span>

nchains = 4                    <span class="comments">;number of markov chains</span>
checkiter = 100               <span class="comments">;check for convergence every 100 iterations</span>
iter = 0L

<span class="comments">;use BCES estimator for initial guess of theta = (alpha, beta, sigsqr)</span>
beta = ( correlate(x, y, /covar) - mean(xycov) ) / $
  ( variance(x) - mean(xvar) )
alpha = mean(y) - beta * mean(x)

sigsqr = variance(y) - mean(yvar) - beta * (correlate(x,y, /covar) - mean(xycov))
sigsqr = sigsqr > 0.05 * variance(y - alpha - beta * x)

                                <span class="comments">;get initial guess of mixture</span>
                                <span class="comments">;parameters prior</span>
mu0 = median(x)
wsqr = variance(x) - median(xvar)
wsqr = wsqr > 0.01 * variance(x)

<span class="comments">;now get MCMC starting values dispersed around these initial guesses</span>

Xmat = [[replicate(1d, nx)], [x]]
Vcoef = invert( Xmat ## transpose(Xmat), /double ) * sigsqr

coef = mrandomn(seed, Vcoef, nchains)
chisqr = randomchi(seed, 4, nchains)

<span class="comments">;randomly disperse starting values for (alpha,beta) from a</span>
<span class="comments">;multivariate students-t distribution with 4 degrees of freedom</span>
alphag = alpha + coef[*,0] * sqrt(4d / chisqr)
betag = beta + coef[*,1] * sqrt(4d / chisqr)

                                <span class="comments">;draw sigsqr from an Inverse scaled</span>
                                <span class="comments">;chi-square density</span>
sigsqrg = sigsqr * (nx / 2) / randomchi(seed, nx / 2, nchains)

<span class="comments">;get starting values for the mixture parameters, first do prior</span>
<span class="comments">;parameters</span>
   
                                <span class="comments">;mu0 is the global mean</span>

mu0min = min(x) <span class="comments">;prior for mu0 is uniform over mu0min &lt; mu0 &lt; mu0max</span>
mu0max = max(x)

repeat begin
    
    mu0g = mu0 + sqrt(variance(x) / nx) * randomn(seed, nchains) / $
      sqrt(4d / randomchi(seed, 4, nchains))

    pass = where(mu0g gt mu0min and mu0g lt mu0max, npass)

endrep until npass eq nchains

                                <span class="comments">;wsqr is the global scale</span>
wsqrg = wsqr * (nx / 2) / randomchi(seed, nx / 2, nchains)

usqrg = replicate(variance(x) / 2d, nchains)

<span class="comments">;now get starting values for mixture parameters</span>

tausqrg = dblarr(ngauss, nchains) <span class="comments">;initial group variances</span>
for k = 0, ngauss - 1 do tausqrg[k,*] = 0.5 * wsqrg * 4 / $
  randomchi(seed, 4, nchains)

mug = dblarr(ngauss, nchains)   <span class="comments">;initial group means</span>
for k = 0, ngauss - 1 do mug[k,*] = mu0g + sqrt(wsqrg) * randomn(seed, nchains)

<span class="comments">;get initial group proportions and group labels</span>

pig = dblarr(ngauss, nchains)
Glabel = intarr(nx, nchains)

if ngauss eq 1 then Glabel = intarr(nx, nchains) else begin

    for i = 0, nchains - 1 do begin
        
        for j = 0, nx - 1 do begin
                                <span class="comments">;classify sources to closest centroid</span>
            dist = abs(mug[*,i] - x[j])
            mindist = min(dist, minind)
            
            pig[minind,i] = pig[minind,i] + 1
            Glabel[j,i] = minind
            
        endfor

    endfor

endelse
                                <span class="comments">;get initial values for pi from a</span>
                                <span class="comments">;dirichlet distribution, with</span>
                                <span class="comments">;parameters based on initial class</span>
                                <span class="comments">;occupancies</span>
if ngauss eq 1 then pig = transpose(replicate(1d, nchains)) else $
  for i = 0, nchains - 1 do pig[*,i] = randomdir(seed, pig[*,i] + 1)

alpha = alphag
beta = betag
sigsqr = sigsqrg
mu = mug
tausqr = tausqrg
pi = pig
mu0 = mu0g
wsqr = wsqrg
usqr = usqrg

eta = dblarr(nx, nchains)
for i = 0, nchains - 1 do eta[*,i] = y <span class="comments">;initial values for eta</span>

nut = 1 <span class="comments">;degrees of freedom for the prior on tausqr</span>
nuu = 1 <span class="comments">;degrees of freedom for the prior on usqr</span>

<span class="comments">;number of parameters to moniter convergence on</span>
npar = 6

if metro then begin
<span class="comments">;get initial variances for the jumping kernels</span>

    jvar_coef = Vcoef
    log_ssqr = alog( sigsqr[0] * nx / randomchi(seed, nx, 1000) )
    jvar_ssqr = variance(log_ssqr) <span class="comments">;get variance of the jumping density</span>
                                   <span class="comments">;for sigsqr</span>

                                <span class="comments">;get variances for prior variance</span>
                                <span class="comments">;parameters</span>
    jvar_mu0 = variance(x) / ngauss
    jvar_wsqr = variance( alog(variance(x) * nx / randomchi(seed, nx, 1000)) )
    jvar_usqr = jvar_wsqr

    naccept = lonarr(5 + 2 * ngauss)
    
    logpost = dblarr(nchains)
                                <span class="comments">;get initial values of the</span>
                                <span class="comments">;log-posterior</span>
    for i = 0, nchains - 1 do begin
        
        theta = [alpha[i], beta[i], sigsqr[i]]
        loglik = loglik_mixerr( x, y, xvar, yvar, xycov, delta, theta, $
                                pi[*,i], mu[*,i], tausqr[*,i], Glabel[*,i] )
        logprior = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])
        logpost[i] = loglik + logprior
        
    endfor
    
endif

convergence = 0

<span class="comments">;stop burn-in phase after BURNSTOP iterations if doing</span>
<span class="comments">;Metropolis-Hastings jumps, update jumping kernels every BURNITER</span>
<span class="comments">;iterations</span>

burnin = metro ? 1 : 0
burniter = 250
burnstop = 500 &lt<span class="comments">; (miniter / 2 > 100)</span>
                                <span class="comments">;start Markov Chains</span>
if not silent then print, 'Simulating Markov Chains...'
if not silent and metro then print, 'Doing Burn-in First...'

ygibbs = y
xi = x
umax = 1.5 * variance(x) <span class="comments">;prior for usqr is uniform over 0 &lt; usqr &lt; umax</span>

if metro then begin
                                <span class="comments">;define arrays now so we don't have to</span>
                                <span class="comments">;create them every MCMC iteration</span>
    Sigma11 = dblarr(nx, ngauss)
    Sigma12 = dblarr(nx, ngauss)
    Sigma22 = dblarr(nx, ngauss)
    determ = dblarr(nx, ngauss)

endif

gamma = dblarr(nx, ngauss)
nk = fltarr(ngauss)

repeat begin
    
    for i = 0, nchains - 1 do begin <span class="comments">;do markov chains one at-a-time</span>
        
        if gibbs then begin
            
            if ncens gt 0 then begin
                                <span class="comments">;first get new values of censored y</span>
                for j = 0, ncens - 1 do begin
                    
                    next = 0
                    repeat ygibbs[cens[j]] = eta[cens[j],i] + $
                      sqrt(yvar[cens[j]]) * randomn(seed) $
                      until ygibbs[cens[j]] le y[cens[j]]
                    
                endfor
                
            endif
            
<span class="comments">;need to get new values of Xi and Eta for Gibbs sampler</span>
            
            if nxerr gt 0 then begin
                                <span class="comments">;first draw Xi|theta,x,y,G,mu,tausqr</span>
                xixy = x[xerr] + xycov[xerr] / yvar[xerr] * (eta[xerr,i] - ygibbs[xerr])
                if nynoerr2 gt 0 then xixy[ynoerr2] = x[xerr[ynoerr2]]
                xixyvar = xvar[xerr] * (1 - xycorr[xerr]^2)
                
                for k = 0, ngauss - 1 do begin <span class="comments">;do one gaussian at-a-time</span>
                    
                    group = where(Glabel[xerr,i] eq k, ngroup)
                    
                    if ngroup gt 0 then begin
                        
                        xihvar = 1d / (beta[i]^2 / sigsqr[i] + 1d / xixyvar[group] + $
                                       1d / tausqr[k,i])
                        xihat = xihvar * $
                          (xixy[group] / xixyvar[group] + $
                           beta[i] * (eta[xerr[group],i] - alpha[i]) / sigsqr[i] + $
                           mu[k,i] / tausqr[k,i])
                        
                        xi[xerr[group]] = xihat + sqrt(xihvar) * randomn(seed, ngroup)
                        
                    endif
                    
                endfor

            endif
            
            if nyerr gt 0 then begin
                                <span class="comments">;now draw Eta|Xi,x,y,theta</span>
                etaxyvar = yvar[yerr] * (1d - xycorr[yerr]^2)
                etaxy = ygibbs[yerr] + xycov[yerr] / xvar[yerr] * (xi[yerr] - x[yerr])
                if nxnoerr2 gt 0 then etaxy[xnoerr2] = ygibbs[yerr[xnoerr2]]
                etahvar = 1d / (1d / sigsqr[i] + 1d / etaxyvar)
                etahat = etahvar * (etaxy / etaxyvar + $
                                    (alpha[i] + beta[i] * xi[yerr]) / sigsqr[i])
                
                eta[yerr,i] = etahat + sqrt(etahvar) * randomn(seed, nyerr)

            endif

        endif

                                <span class="comments">;now draw new class labels</span>
        if ngauss eq 1 then Glabel[*,i] = 0 else begin
            
            if gibbs then begin
                                <span class="comments">;get unnormalized probability that</span>
                                <span class="comments">;source i came from Gaussian k, given</span>
                                <span class="comments">;xi[i]</span>
                for k = 0, ngauss - 1 do $  
                  gamma[0,k] = pi[k,i] / sqrt(2d * !pi * tausqr[k,i]) * $
                  exp(-0.5 * (xi - mu[k,i])^2 / tausqr[k,i])
                
            endif else begin
                
                for k = 0, ngauss - 1 do begin
                    
                    Sigma11[0,k] = beta[i]^2 * tausqr[k,i] + sigsqr[i] + yvar
                    Sigma12[0,k] = beta[i] * tausqr[k,i] + xycov
                    Sigma22[0,k] = tausqr[k,i] + xvar
                    
                    determ[0, k] = Sigma11[*,k] * Sigma22[*,k] - Sigma12[*,k]^2
                    
                endfor
                
                if ndet gt 0 then begin
                                <span class="comments">;get unnormalized probability that</span>
                                <span class="comments">;source i came from Gaussian k, given</span>
                                <span class="comments">;x[i] and y[i]</span>
                    for k = 0, ngauss - 1 do begin
                        
                        zsqr = (y[det] - alpha[i] - beta[i] * mu[k,i])^2 / Sigma11[det,k] + $
                          (x[det] - mu[k,i])^2 / Sigma22[det,k] - $
                          2d * Sigma12[det,k] * (y[det] - alpha[i] - beta[i] * mu[k,i]) * $
                              (x[det] - mu[k,i]) / (Sigma11[det,k] * Sigma22[det,k])
                        
                        corrz = Sigma12[det,k] / sqrt( Sigma11[det,k] * Sigma22[det,k] )
                        
                        lognorm = -0.5d * alog(determ[det,k]) - 0.5 * zsqr / (1d - corrz^2)
                        
                        gamma[det,k] = pi[k,i] * exp(lognorm) / (2d * !pi)
                            
                    endfor
                    
                endif
                
                if ncens gt 0 then begin
                                <span class="comments">;get unnormalized probability that</span>
                                <span class="comments">;source i came from Gaussian k, given</span>
                                <span class="comments">;x[i] and y[i] > y0[i]</span>
                    for k = 0, ngauss - 1 do begin
                        
                        gamma[cens,k] = pi[k,i] / sqrt(2d * !pi * Sigma22[cens,k]) * $
                          exp(-0.5 * (x[cens] - mu[k,i])^2 / Sigma22[cens,k])
                        
                                <span class="comments">;conditional mean of y, given x</span>
                        cmeany = alpha[i] + beta[i] * mu[k,i] + Sigma12[cens,k] / Sigma22[cens,k] * $
                          (x[cens] - mu[k,i])
                                <span class="comments">;conditional variance of y, given x</span>
                        cvary = Sigma11[cens,k] - Sigma12[cens,k]^2 / Sigma22[cens,k]
                                <span class="comments">;make sure logliky is finite</span>
                        gamma[cens,k] = gamma[cens,k] * gauss_pdf( (y[cens] - cmeany) / sqrt(cvary) )
                        
                    endfor
                    
                endif
                
            endelse
            
            norm = total(gamma, 2)
            
            for j = 0, nx - 1 do begin
                
                gamma0 = reform(gamma[j,*]) / norm[j] <span class="comments">;normalized probability that the i-th data point </span>
                                                      <span class="comments">;is from the k-th Gaussian, given the observed</span>
                                                      <span class="comments">;data point </span>
                Gjk = multinom(1, gamma0, seed=seed)
                Glabel[j,i] = where(Gjk eq 1)
                
            endfor
            
        endelse
        
<span class="comments">;now draw new values of regression parameters, theta = (alpha, beta,</span>
<span class="comments">;sigsqr)</span>
        
        if gibbs then begin
                                <span class="comments">;use gibbs sampler to draw alpha,beta|Xi,Eta,sigsqr</span>
            Xmat = [[replicate(1d, nx)], [xi]]
            Vcoef = invert( Xmat ## transpose(Xmat), /double ) * sigsqr[i]
            
            coefhat = linfit( xi, eta[*,i] )
            coef = coefhat + mrandomn(seed, Vcoef)
            
            alpha[i] = coef[0]
            beta[i] = coef[1]
            
        endif else begin

            theta = [alpha[i], beta[i], sigsqr[i]]

            loglik = loglik_mixerr( x, ygibbs, xvar, yvar, xycov, delta, theta, $
                                    pi[*,i], mu[*,i], tausqr[*,i], Glabel[*,i] )
            logprior = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])

            logpost[i] = loglik + logprior <span class="comments">;log-posterior for current parameter values</span>

                                <span class="comments">;use metropolis update to get new</span>
                                <span class="comments">;values of the coefficients</span>
            coef = [alpha[i], beta[i]] + mrandomn(seed, jvar_coef)
            
            theta = [coef[0], coef[1], sigsqr[i]]
            loglik_new = loglik_mixerr( x, ygibbs, xvar, yvar, xycov, delta, theta, $
                                        pi[*,i], mu[*,i], tausqr[*,i], Glabel[*,i] )
            logprior_new = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])
            
            logpost_new = loglik_new + logprior_new
            
            accept = linmix_metro_update( logpost_new, logpost[i], seed )
            
            if accept then begin
                
                naccept[0] = naccept[0] + 1L
                alpha[i] = coef[0]
                beta[i] = coef[1]
                logpost[i] = logpost_new
                
            endif
            
        endelse
                                <span class="comments">;now get sigsqr</span>
        if gibbs then begin
            
            ssqr = total( (eta[*,i] - alpha[i] - beta[i] * xi)^2 ) / (nx - 2)
            sigsqr[i] = (nx - 2) * ssqr / randomchi(seed, nx - 2.0)
            
        endif else begin
                                <span class="comments">;do metropolis update</span>
            log_ssqr = alog(sigsqr[i]) + sqrt(jvar_ssqr) * randomn(seed)
            ssqr = exp(log_ssqr)
            
            theta = [alpha[i], beta[i], ssqr]
            
            loglik_new = loglik_mixerr( x, ygibbs, xvar, yvar, xycov, delta, theta, $
                                        pi[*,i], mu[*,i], tausqr[*,i], Glabel[*,i] )
            logprior_new = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])
            
            logpost_new = loglik_new + logprior_new + log_ssqr
            logpost_old = logpost[i] + alog(sigsqr[i])
            
            accept = linmix_metro_update( logpost_new, logpost_old, seed )
            
            if accept then begin
                
                naccept[1] = naccept[1] + 1L
                sigsqr[i] = ssqr
                logpost[i] = loglik_new + logprior_new
                
            endif
            
        endelse 

<span class="comments">;now do mixture model parameters, psi = (pi,mu,tausqr)</span>
        
        if gibbs then begin

            for k = 0, ngauss - 1 do begin 

                group = where(Glabel[*,i] eq k, ngroup)
                nk[k] = ngroup

                if ngroup gt 0 then begin

                                <span class="comments">;get mu|Xi,G,tausqr,mu0,usqr</span>

                    if ngauss gt 1 then begin

                        muhat = ngroup * mean(xi[group]) / tausqr[k,i] + mu0[i] / usqr[i]
                        
                        muvar = 1d / (1d / usqr[i] + ngroup / tausqr[k,i])

                    endif else begin

                        muhat = ngroup * mean(xi[group]) / tausqr[k,i]

                        muvar = tausqr[k,i] / ngroup

                    endelse

                    muhat = muvar * muhat
                    
                    mu[k,i] = muhat + sqrt(muvar) * randomn(seed)
                    
                                <span class="comments">;get tausqr|Xi,G,mu,wsqr,nut</span>

                    if ngauss gt 1 then begin
                        
                        nuk = ngroup + nut
                        tsqr = (nut * wsqr[i] + total( (xi[group] - mu[k,i])^2 )) / nuk
                        
                    endif else begin

                        nuk = ngroup
                        tsqr = total( (xi[group] - mu[k,i])^2 ) / nuk

                    endelse
                
                    tausqr[k,i] = tsqr * nuk / randomchi(seed, nuk)

                endif else begin

                    mu[k,i] = mu0[i] + sqrt(usqr[i]) * randomn(seed)
                    tausqr[k,i] = wsqr[i] * nut / randomchi(seed, nut)

                endelse

            endfor
                                <span class="comments">;get pi|G</span>
            if ngauss eq 1 then pi[*,i] = 1d else $
              pi[*,i] = randomdir(seed, nk + 1)

        endif else begin
                                <span class="comments">;do metropolis-hastings updating using</span>
                                <span class="comments">;approximate Gibbs sampler</span>

            for k = 0, ngauss - 1 do begin
                
                group = where(Glabel[*,i] eq k, ngroup)
                nk[k] = ngroup

                if ngroup gt 0 then begin
                                <span class="comments">;get proposal for mu[k], do</span>
                                <span class="comments">;approximate Gibbs sampler</span>
                    muprop = mu[*,i]

                    muvarx = (tausqr[k,i] + mean(xvar[group]))

                    muvar = ngauss gt 1 ? 1d / (1d / usqr[i] + ngroup / muvarx) : $
                      muvarx / ngroup

                    muhat = muprop
                    
                    chisqr = randomchi(seed, 4)
                                <span class="comments">;draw proposal for mu from Student's t</span>
                                <span class="comments">;with 4 degrees of freedom</span>
                    muprop[k] = muhat[k] + sqrt(muvar * 4 / chisqr) * randomn(seed)

                endif else begin
                    
                    muprop = mu[*,i]
                    muprop[k] = mu[k,i] + sqrt(usqr[i]) * randomn(seed)

                endelse

                theta = [alpha[i], beta[i], sigsqr[i]]
                
                loglik_new = loglik_mixerr( x, ygibbs, xvar, yvar, xycov, delta, theta, $
                                            pi[*,i], muprop, tausqr[*,i], Glabel[*,i] )
                logprior_new = logprior_mixerr(muprop, mu0[i], tausqr[*,i], usqr[i], wsqr[i])
                
                logpost_new = loglik_new + logprior_new
                
                accept = linmix_metro_update( logpost_new, logpost[i], seed )
                
                if accept then begin
                    
                    naccept[2+k] = naccept[2+k] + 1L
                    mu[k,i] = muprop[k]
                    logpost[i] = logpost_new
                    
                endif

                                <span class="comments">;get proposal for tausqr[k], do</span>
                                <span class="comments">;approximate Gibbs sampler</span>
                tsqrprop = tausqr[*,i]
 
                dof = ngroup > 1

                tsqrprop[k] = tausqr[k,i] * dof / randomchi(seed, dof)

                log_jrat = (dof + 1d) * alog(tsqrprop[k] / tausqr[k,i]) + $
                  dof / 2d * (tausqr[k,i] / tsqrprop[k] - tsqrprop[k] / tausqr[k,i])

                loglik_new = loglik_mixerr( x, ygibbs, xvar, yvar, xycov, delta, theta, $
                                            pi[*,i], mu[*,i], tsqrprop, Glabel[*,i] )
                logprior_new = logprior_mixerr(mu[*,i], mu0[i], tsqrprop, usqr[i], wsqr[i])
                
                logpost_new = loglik_new + logprior_new
                
                accept = linmix_metro_update( logpost_new, logpost[i], seed, log_jrat)
                
                if accept then begin
                    
                    naccept[2 + k + ngauss] = naccept[2 + k + ngauss] + 1L
                    tausqr[k,i] = tsqrprop[k]
                    logpost[i] = logpost_new
                    
                endif
                
            endfor
                                <span class="comments">;get pi|G, can do exact Gibbs sampler</span>
                                <span class="comments">;for this</span>
            if ngauss eq 1 then pi[*,i] = 1d else $
              pi[*,i] = randomdir(seed, nk + 1)

        endelse
        
<span class="comments">;finally, update parameters for prior distribution, only do this if</span>
<span class="comments">;more than one gaussian</span>
        
        if ngauss gt 1 then begin

            if gibbs then begin
                
                repeat mu0[i] = mean(mu[*,i]) + sqrt(usqr[i] / ngauss) * randomn(seed) $
                  until (mu0[i] gt mu0min) and (mu0[i] lt mu0max)

            endif else begin
                
                loglik = loglik_mixerr( x, ygibbs, xvar, yvar, xycov, delta, theta, $
                                        pi[*,i], mu[*,i], tausqr[*,i], Glabel[*,i] )
                
                muprop = mu0[i] + sqrt(jvar_mu0) * randomn(seed)

                if muprop gt mu0min and muprop lt mu0max then begin

                    logprior_old = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])
                    logprior_new = logprior_mixerr(mu[*,i], muprop, tausqr[*,i], usqr[i], wsqr[i])
                    
                    logpost_new = loglik + logprior_new
                    logpost_old = loglik + logprior_old
                    
                    accept = linmix_metro_update( logpost_new, logpost_old, seed )
                    
                    if accept then begin
                        
                        naccept[2 + 2 * ngauss] = naccept[2 + 2 * ngauss] + 1L
                        mu0[i] = muprop
                        logpost[i] = loglik + logprior_new
                        
                    endif
                
                endif

            endelse

            if gibbs then begin
                
                nu = ngauss + nuu
                usqr0 = (nuu * wsqr[i] + total( (mu[*,i] - mu0[i])^2 )) / nu
                
                repeat usqr[i] = usqr0 * nu / randomchi(seed, nu) $
                  until usqr[i] le umax
                
            endif else begin
                                <span class="comments">;do metropolis update</span>

                log_usqr = alog(usqr[i]) + sqrt(jvar_usqr) * randomn(seed)
                usqr0 = exp(log_usqr)
                
                if usqr0 le umax then begin

                    logprior_old = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])
                    
                    logpost[i] = loglik + logprior_old <span class="comments">;update posterior after gibbs step for mu0</span>
                    
                    logprior_new = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr0, wsqr[i])
                    
                    logpost_new = loglik + logprior_new
                    logpost_old = loglik + logprior_old
                    
                    log_jrat = log_usqr - alog(usqr[i])
                    
                    accept = linmix_metro_update( logpost_new, logpost_old, seed, log_jrat )
                    
                    if accept then begin
                        
                        naccept[3 + 2 * ngauss] = naccept[3 + 2 * ngauss] + 1L
                        usqr[i] = usqr0
                        logpost[i] = loglik + logprior_new
                        
                    endif

                endif
                
            endelse
            
            if gibbs then begin
                
                alphaw = ngauss * nut / 2d + 1
                betaw = 0.5 * nut * total(1d / tausqr[*,i])
                
                wsqr[i] = randomgam(seed, alphaw, betaw)
                
            endif else begin
                
                log_wsqr = alog(wsqr[i]) + sqrt(jvar_wsqr) * randomn(seed)
                wsqr0 = exp(log_wsqr)
                
                logprior_old = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr[i])
                logprior_new = logprior_mixerr(mu[*,i], mu0[i], tausqr[*,i], usqr[i], wsqr0)
                
                logpost_new = loglik + logprior_new + log_wsqr
                logpost_old = loglik + logprior_old + alog(wsqr[i])
                
                accept = linmix_metro_update( logpost_new, logpost_old, seed )
                
                if accept then begin
                    
                    naccept[4 + 2 * ngauss] = naccept[4 + 2 * ngauss] + 1L
                    wsqr[i] = wsqr0
                    logpost[i] = loglik + logprior_new
                    
                endif
                
            endelse

        endif

    endfor

                                <span class="comments">;save Markov Chains</span>
    if iter eq 0 then begin
        
        alphag = alpha
        betag = beta
        sigsqrg = sigsqr
        
        pig = pi
        mug = mu
        tausqrg = tausqr

        if ngauss gt 1 then begin

            mu0g = mu0
            usqrg = usqr
            wsqrg = wsqr
        
        endif

        if metro then logpostg = logpost

    endif else begin
        
        alphag = [alphag, alpha]
        betag = [betag, beta]
        sigsqrg = [sigsqrg, sigsqr]

        pig = [[pig], [pi]]
        mug = [[mug], [mu]]
        tausqrg = [[tausqrg], [tausqr]]
        
        if ngauss gt 1 then begin
        
            mu0g = [mu0g, mu0]
            usqrg = [usqrg, usqr]
            wsqrg = [wsqrg, wsqr]

        endif
        
        if metro then logpostg = [logpostg, logpost]

    endelse
        
    iter = iter + 1L
    
<span class="comments">;check for convergence</span>
    
    if iter ge 4 and iter eq checkiter and not burnin then begin

        if not silent and metro then linmix_metro_results, $
          float(naccept) / (nchains * iter), ngauss

        Bvar = dblarr(npar)     <span class="comments">;between-chain variance</span>
        Wvar = dblarr(npar)     <span class="comments">;within-chain variance</span>
        
        psi = dblarr(iter, nchains, npar)
        
        psi[*,*,0] = transpose(reform(alphag, nchains, iter))
        psi[*,*,1] = transpose(reform(betag, nchains, iter))
        psi[*,*,2] = transpose(reform(sigsqrg, nchains, iter))

        pig2 = reform(pig, ngauss, nchains, iter)
        mug2 = reform(mug, ngauss, nchains, iter)
        tausqrg2 = reform(tausqrg, ngauss, nchains, iter)

        psi[*,*,3] = transpose( total(pig2 * mug2, 1) ) <span class="comments">;mean of xi</span>
                                <span class="comments">;variance of xi</span>
        psi[*,*,4] = transpose( total(pig2 * (tausqrg2 + mug2^2), 1) ) - psi[*,*,3]^2
                                <span class="comments">;linear correlation coefficient</span>
                                <span class="comments">;between xi and eta</span>
        psi[*,*,5] = psi[*,*,1] * sqrt(psi[*,*,4] / (psi[*,*,1]^2 * psi[*,*,4] + psi[*,*,2]))
                                <span class="comments">;do normalizing transforms before</span>
                                <span class="comments">;monitering convergence</span>
        psi[*,*,2] = alog(psi[*,*,2])
        psi[*,*,4] = alog(psi[*,*,4])
        psi[*,*,5] = linmix_atanh(psi[*,*,5])

        psi = psi[iter/2:*,*,*] <span class="comments">;discard first half of MCMC</span>

        ndraw = iter / 2
                                <span class="comments">;calculate between- and within-sequence</span>
                                <span class="comments">;                  variances</span>
        for j = 0, npar - 1 do begin
            
            psibarj = total( psi[*,*,j], 1 ) / ndraw
            psibar = mean(psibarj)
            
            sjsqr = 0d
            for i = 0, nchains - 1 do $
              sjsqr = sjsqr + total( (psi[*, i, j] - psibarj[i])^2 ) / (ndraw - 1.0)
            
            Bvar[j] = ndraw / (nchains - 1.0) * total( (psibarj - psibar)^2 )
            Wvar[j] = sjsqr / nchains
            
        endfor
        
        varplus = (1.0 - 1d / ndraw) * Wvar + Bvar / ndraw
        Rhat = sqrt( varplus / Wvar ) <span class="comments">;potential variance scale reduction factor</span>
        
        if total( (Rhat le 1.1) ) eq npar and iter ge miniter then convergence = 1 $
        else if iter ge maxiter then convergence = 1 else begin
            
            if not silent then begin
                print, 'Iteration: ', iter
                print, 'Rhat Values for ALPHA, BETA, log(SIGMA^2), mean(XI), ' + $
                  'log(variance(XI), atanh(corr(XI,ETA)) ): '
                print, Rhat
            endif
            
            checkiter = checkiter + 100L
            
        endelse

    endif
    
    if (burnin) and (iter eq burniter) then begin
<span class="comments">;still doing burn-in stage, get new estimates for jumping kernel</span>
<span class="comments">;parameters</span>
        
        jvar_ssqr = linmix_robsig( alog(sigsqrg) )^2

                                <span class="comments">;now modify covariance matrix for</span>
                                <span class="comments">;coefficient jumping kernel</span>
        coefg = [[alphag], [betag]]
        
        jvar_coef = correlate( transpose(coefg), /covar)

        if ngauss gt 1 then begin

            jvar_mu0 = linmix_robsig(mu0g)^2 * 2.4^2

            jvar_usqr = linmix_robsig( alog(usqrg) )^2 * 2.4^2
    
            jvar_wsqr = linmix_robsig( alog(wsqrg) )^2 * 2.4^2

        endif
        
        if iter eq burnstop then burnin = 0
        
        if not burnin then begin

           if not silent then print, 'Burn-in Complete'

           iter = 0L

        endif

        naccept = lonarr(5 + 2 * ngauss)
        burniter = burniter + 250L
        
    endif
    
endrep until convergence

ndraw = iter * nchains / 2

<span class="comments">;save posterior draws in a structure</span>

if ngauss gt 1 then begin

    post = {alpha:0d, beta:0d, sigsqr:0d, pi:dblarr(ngauss), mu:dblarr(ngauss), $
            tausqr:dblarr(ngauss), mu0:0d, usqr:0d, wsqr:0d, ximean:0d, xisig:0d, $
            corr:0d}

endif else begin
    
    post = {alpha:0d, beta:0d, sigsqr:0d, pi:dblarr(ngauss), mu:dblarr(ngauss), $
            tausqr:dblarr(ngauss), ximean:0d, xisig:0d, corr:0d}

endelse

post = replicate(post, ndraw)

post.alpha = alphag[(iter*nchains+1)/2:*]
post.beta = betag[(iter*nchains+1)/2:*]
post.sigsqr = sigsqrg[(iter*nchains+1)/2:*]
post.pi = pig[*,(iter*nchains+1)/2:*]
post.mu = mug[*,(iter*nchains+1)/2:*]
post.tausqr = tausqrg[*,(iter*nchains+1)/2:*]

if ngauss gt 1 then begin

    post.mu0 = mu0g[(iter*nchains+1)/2:*]
    post.usqr = usqrg[(iter*nchains+1)/2:*]
    post.wsqr = wsqrg[(iter*nchains+1)/2:*]

endif

post.ximean = total(post.pi * post.mu, 1) <span class="comments">;mean of xi   </span>
post.xisig = total(post.pi * (post.tausqr + post.mu^2), 1) - post.ximean^2
post.xisig = sqrt(post.xisig)   <span class="comments">;standard deviation of xi</span>

                                <span class="comments">;get linear correlation coefficient</span>
                                <span class="comments">;between xi and eta</span>
post.corr = post.beta * post.xisig / sqrt(post.beta^2 * post.xisig^2 + post.sigsqr)

return
end
</code>
    </div>
  </body>
</html>