<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<!-- Generated by IDLdoc 3.5.1 on Mon Sep 30 16:57:48 2013 -->

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
    <title>mlinmix_err.pro (Documentation for ./)</title>

    
    <link rel="stylesheet" type="text/css" media="all"
          href="../idldoc-resources/main.css" />
    <link rel="stylesheet" type="text/css" media="print"
          href="../idldoc-resources/main-print.css" />
    

    <script type="text/javascript">
      function setTitle() {
        parent.document.title="mlinmix_err.pro (Documentation for ./)";
      }
    </script>
  </head>

  <body onload="setTitle();" id="root">
    <div class="content">
      <code class="source"><span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;+</span>
<span class="comments">;   NAME:</span>
<span class="comments">;     MLINMIX_ERR</span>
<span class="comments">;   PURPOSE:</span>
<span class="comments">;      Bayesian approach to multiple linear regression with errors in  X and Y</span>
<span class="comments">;   EXPLANATION:</span>
<span class="comments">; PERFORM LINEAR REGRESSION OF Y ON X WHEN THERE ARE MEASUREMENT</span>
<span class="comments">; ERRORS IN BOTH VARIABLES. THE REGRESSION ASSUMES :</span>
<span class="comments">;</span>
<span class="comments">;                 ETA = ALPHA + BETA ## XI + EPSILON</span>
<span class="comments">;                 X = XI + XERR</span>
<span class="comments">;                 Y = ETA + YERR</span>
<span class="comments">;</span>
<span class="comments">; HERE, (ALPHA, BETA) ARE THE REGRESSION COEFFICIENTS, EPSILON IS THE</span>
<span class="comments">; INTRINSIC RANDOM SCATTER ABOUT THE REGRESSION, XERR IS THE</span>
<span class="comments">; MEASUREMENT ERROR IN X, AND YERR IS THE MEASUREMENT ERROR IN</span>
<span class="comments">; Y. EPSILON IS ASSUMED TO BE NORMALLY-DISTRIBUTED WITH MEAN ZERO AND</span>
<span class="comments">; VARIANCE SIGSQR. XERR AND YERR ARE ASSUMED TO BE</span>
<span class="comments">; NORMALLY-DISTRIBUTED WITH MEANS EQUAL TO ZERO, COVARIANCE MATRICES</span>
<span class="comments">; XVAR^2 FOR X, VARIANCES YSIG^2 FOR Y, AND COVARIANCE VECTORS</span>
<span class="comments">; XYCOV. THE DISTRIBUTION OF XI IS MODELLED AS A MIXTURE OF NORMALS,</span>
<span class="comments">; WITH GROUP PROPORTIONS PI, MEANS MU, AND COVARIANCES T. BAYESIAN</span>
<span class="comments">; INFERENCE IS EMPLOYED, AND A STRUCTURE CONTAINING RANDOM DRAWS FROM</span>
<span class="comments">; THE POSTERIOR IS RETURNED. CONVERGENCE OF THE MCMC TO THE POSTERIOR</span>
<span class="comments">; IS MONITORED USING THE POTENTIAL SCALE REDUCTION FACTOR (RHAT,</span>
<span class="comments">; GELMAN ET AL.2004). IN GENERAL, WHEN RHAT &lt; 1.1 THEN APPROXIMATE</span>
<span class="comments">; CONVERGENCE IS REACHED.</span>
<span class="comments">;</span>
<span class="comments">; SIMPLE NON-DETECTIONS ON Y MAY ALSO BE INCLUDED</span>
<span class="comments">;</span>
<span class="comments">; AUTHOR : BRANDON C. KELLY, STEWARD OBS., JULY 2006</span>
<span class="comments">;</span>
<span class="comments">; INPUTS :</span>
<span class="comments">;</span>
<span class="comments">;   X - THE OBSERVED INDEPENDENT VARIABLES. THIS SHOULD BE AN</span>
<span class="comments">;       [NX, NP]-ELEMENT ARRAY.</span>
<span class="comments">;   Y - THE OBSERVED DEPENDENT VARIABLE. THIS SHOULD BE AN NX-ELEMENT</span>
<span class="comments">;       VECTOR.</span>
<span class="comments">;</span>
<span class="comments">; OPTIONAL INPUTS :</span>
<span class="comments">;</span>
<span class="comments">;   XVAR - THE COVARIANCE MATRIX OF THE X ERRORS, AND</span>
<span class="comments">;          [NX,NP,NP]-ELEMENT ARRAY. XVAR[I,*,*] IS THE COVARIANCE</span>
<span class="comments">;          MATRIX FOR THE ERRORS ON X[I,*]. THE DIAGONAL OF</span>
<span class="comments">;          XVAR[I,*,*] MUST BE GREATER THAN ZERO FOR EACH DATA POINT.</span>
<span class="comments">;   YVAR - THE VARIANCE OF THE Y ERRORS, AND NX-ELEMENT VECTOR. YVAR</span>
<span class="comments">;          MUST BE GREATER THAN ZERO.</span>
<span class="comments">;   XYCOV - THE VECTOR OF COVARIANCES FOR THE MEASUREMENT ERRORS</span>
<span class="comments">;           BETWEEN X AND Y.</span>
<span class="comments">;   DELTA - AN NX-ELEMENT VECTOR INDICATING WHETHER A DATA POINT IS</span>
<span class="comments">;           CENSORED OR NOT. IF DELTA[i] = 1, THEN THE SOURCE IS</span>
<span class="comments">;           DETECTED, ELSE IF DELTA[i] = 0 THE SOURCE IS NOT DETECTED</span>
<span class="comments">;           AND Y[i] SHOULD BE AN UPPER LIMIT ON Y[i]. NOTE THAT IF</span>
<span class="comments">;           THERE ARE CENSORED DATA POINTS, THEN THE</span>
<span class="comments">;           MAXIMUM-LIKELIHOOD ESTIMATE (THETA) IS NOT VALID. THE</span>
<span class="comments">;           DEFAULT IS TO ASSUME ALL DATA POINTS ARE DETECTED, IE,</span>
<span class="comments">;           DELTA = REPLICATE(1, NX).</span>
<span class="comments">;   SILENT - SUPPRESS TEXT OUTPUT.</span>
<span class="comments">;   MINITER - MINIMUM NUMBER OF ITERATIONS PERFORMED BY THE GIBBS</span>
<span class="comments">;             SAMPLER. IN GENERAL, MINITER = 5000 SHOULD BE SUFFICIENT</span>
<span class="comments">;             FOR CONVERGENCE. THE DEFAULT IS MINITER = 5000. THE</span>
<span class="comments">;             GIBBS SAMPLER IS STOPPED AFTER RHAT &lt; 1.1 FOR ALPHA,</span>
<span class="comments">;             BETA, AND SIGMA^2, AND THE NUMBER OF ITERATIONS</span>
<span class="comments">;             PERFORMED IS GREATER THAN MINITER.</span>
<span class="comments">;   MAXITER - THE MAXIMUM NUMBER OF ITERATIONS PERFORMED BY THE</span>
<span class="comments">;             MCMC. THE DEFAULT IS 1D5. THE GIBBS SAMPLER IS STOPPED</span>
<span class="comments">;             AUTOMATICALLY AFTER MAXITER ITERATIONS.</span>
<span class="comments">;   NGAUSS - THE NUMBER OF GAUSSIANS TO USE IN THE MIXTURE</span>
<span class="comments">;            MODELLING. THE DEFAULT IS 3. </span>
<span class="comments">;</span>
<span class="comments">; OUTPUT :</span>
<span class="comments">;</span>
<span class="comments">;    POST - A STRUCTURE CONTAINING THE RESULTS FROM THE GIBBS</span>
<span class="comments">;           SAMPLER. EACH ELEMENT OF POST IS A DRAW FROM THE POSTERIOR</span>
<span class="comments">;           DISTRIBUTION FOR EACH OF THE PARAMETERS.</span>
<span class="comments">;</span>
<span class="comments">;             ALPHA - THE CONSTANT IN THE REGRESSION.</span>
<span class="comments">;             BETA - THE SLOPES OF THE REGRESSION.</span>
<span class="comments">;             SIGSQR - THE VARIANCE OF THE INTRINSIC SCATTER.</span>
<span class="comments">;             PI - THE GAUSSIAN WEIGHTS FOR THE MIXTURE MODEL.</span>
<span class="comments">;             MU - THE GAUSSIAN MEANS FOR THE MIXTURE MODEL.</span>
<span class="comments">;             T - THE GAUSSIAN COVARIANCE MATRICES FOR THE MIXTURE</span>
<span class="comments">;                 MODEL.</span>
<span class="comments">;             MU0 - THE HYPERPARAMETER GIVING THE MEAN VALUE OF THE</span>
<span class="comments">;                   GAUSSIAN PRIOR ON MU.</span>
<span class="comments">;             U - THE HYPERPARAMETER DESCRIBING FOR THE PRIOR</span>
<span class="comments">;                 COVARIANCE MATRIX OF THE INDIVIDUAL GAUSSIAN</span>
<span class="comments">;                 CENTROIDS ABOUT MU0.</span>
<span class="comments">;             W - THE HYPERPARAMETER DESCRIBING THE `TYPICAL' SCALE</span>
<span class="comments">;                 MATRIX FOR THE PRIOR ON (T,U).</span>
<span class="comments">;             XIMEAN - THE MEAN OF THE DISTRIBUTION FOR THE</span>
<span class="comments">;                      INDEPENDENT VARIABLE, XI.</span>
<span class="comments">;             XIVAR - THE STANDARD COVARIANCE MATRIX FOR THE</span>
<span class="comments">;                     DISTRIBUTION OF THE INDEPENDENT VARIABLE, XI.</span>
<span class="comments">;             XICORR - SAME AS XIVAR, BUT FOR THE CORRELATION MATRIX.</span>
<span class="comments">;             CORR - THE LINEAR CORRELATION COEFFICIENT BETWEEN THE</span>
<span class="comments">;                    DEPENDENT AND INDIVIDUAL INDEPENDENT VARIABLES,</span>
<span class="comments">;                    XI AND ETA.</span>
<span class="comments">;             PCORR - SAME AS CORR, BUT FOR THE PARTIAL CORRELATIONS.</span>
<span class="comments">;</span>
<span class="comments">; CALLED ROUTINES :</span>
<span class="comments">;</span>
<span class="comments">;    RANDOMCHI, MRANDOMN, RANDOMWISH, RANDOMDIR, MULTINOM</span>
<span class="comments">;</span>
<span class="comments">; REFERENCES :</span>
<span class="comments">;</span>
<span class="comments">;   Carroll, R.J., Roeder, K., & Wasserman, L., 1999, Flexible</span>
<span class="comments">;     Parametric Measurement Error Models, Biometrics, 55, 44</span>
<span class="comments">;</span>
<span class="comments">;   Kelly, B.C., 2007, Some Aspects of Measurement Error in</span>
<span class="comments">;     Linear Regression of Astronomical Data, ApJ, In press</span>
<span class="comments">;     (astro-ph/0705.2774)</span>
<span class="comments">;</span>
<span class="comments">;   Gelman, A., Carlin, J.B., Stern, H.S., & Rubin, D.B., 2004,</span>
<span class="comments">;     Bayesian Data Analysis, Chapman & Hall/CRC</span>
<span class="comments">;-</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to compute the inverse of the lower triangular matrix output</span>
<span class="comments">;from the Cholesky decomposition</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="mlinmix_chol_invert:source"></a>function mlinmix_chol_invert, L

n = n_elements(L[*,0])

X = dblarr(n, n) <span class="comments">;X is the matrix inverse of L</span>

for i = 0, n - 1 do begin

    X[i,i] = 1d / L[i,i]

    if i lt n - 1 then begin

        for j = i + 1, n - 1 do begin

            sum = 0d
            for k = i, j - 1 do sum = sum - L[k,j] * X[i,k]
            X[i,j] = sum / L[j,j]

        endfor

    endif

endfor

return, X
end

<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;routine to compute the inverse of a symmetric positive-definite</span>
<span class="comments">;matrix via the Cholesky decomposition</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="mlinmix_posdef_invert:source"></a>pro mlinmix_posdef_invert, A

dim = n_elements(A[*,0])
diag = lindgen(dim) * (dim + 1L)

choldc, A, P, /double

for j = 0, dim - 1 do for k = j, dim - 1 do A[k,j] = 0d

A[diag] = P

A = mlinmix_chol_invert(A)

A = transpose(A) ## A

return
end


<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>
<span class="comments">;                                                                     ;</span>
<span class="comments">;                             MAIN ROUTINE                            ;</span>
<span class="comments">;                                                                     ;</span>
<span class="comments">;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;</span>

<a id="mlinmix_err:source"></a>pro mlinmix_err, x, y, post, xvar=xvar, yvar=yvar, xycov=xycov, silent=silent, $
                 delta=delta, miniter=miniter, maxiter=maxiter, ngauss=ngauss

if n_params() lt 3 then begin

    print, 'Syntax- MLINMIX_ERR, X, Y, POST, XVAR=XVAR, YVAR=YVAR, XYCOV=XYCOV,'
    print, '                    NGAUSS=NGAUSS, /SILENT, DELTA=DELTA, '
    PRINT, '                    MINITER=MINITER, MAXITER=MAXITER'
    return

endif

<span class="comments">;check inputs and setup defaults</span>

nx = size(x)

if nx[0] ne 2 then begin
    print, 'X must be an [NX,NP]-element array.'
    return
endif

np = nx[2]
nx = nx[1]

if n_elements(y) ne nx then begin
    print, 'Y and X must have the same size.'
    return
endif

if n_elements(xvar) eq 0 and n_elements(yvar) eq 0 then begin
    print, 'Must supply at least one of XVAR or YVAR.'
    return
endif

xvar_size = size(xvar)

if (xvar_size[0] ne 3) or (xvar_size[1] ne nx) or (xvar_size[2] ne np) or $
  (xvar_size[3] ne np) then begin
    print, 'XVAR must be an [NX,NP,NP]-element array.'
    return
endif

if n_elements(yvar) ne nx then begin
    print, 'YVAR and Y must have the same size.'
    return
endif

if n_elements(xycov) eq 0 then xycov = dblarr(nx, np)

if n_elements(xycov[*,0]) ne nx or n_elements(xycov[0,*]) ne np then begin
    print, 'XYCOV must be an [NX,NP]-element array.'
    return
endif

if n_elements(delta) eq 0 then delta = replicate(1, nx)
if n_elements(delta) ne nx then begin
    print, 'DELTA and X must have the same size.'
    return
endif

diag = lindgen(np) * (np + 1)
diag2 = lindgen(np+1) * (np + 2)

zero = where(xvar[diag] eq 0 or yvar eq 0, nzero)
if nzero gt 0 then begin
    print, 'Measurement Errors in X and Y have to have non-zero variance.'
    return
endif

det = where(delta eq 1, ndet, comp=cens, ncomp=ncens) <span class="comments">;get detected data points</span>

if not keyword_set(silent) then silent = 0
if n_elements(miniter) eq 0 then miniter = 5000 <span class="comments">;minimum number of iterations that the </span>
                                                <span class="comments">;Markov Chain must perform</span>
if n_elements(maxiter) eq 0 then maxiter = 100000L <span class="comments">;maximum number of iterations that the </span>
                                                   <span class="comments">;Markov Chains will perform</span>

if n_elements(ngauss) eq 0 then ngauss = 3

if ngauss le 0 then begin
    print, 'NGAUSS must be at least 1.'
    return
endif

<span class="comments">;store covariance matrices for (x,y) measurement errors</span>

xyvar = dblarr(nx,np+1,np+1)

xyvar[*,0,0] = yvar
xyvar[*,1:*,0] = xycov
xyvar[*,0,1:*] = xycov
xyvar[*,1:*,1:*] = xvar

<span class="comments">;; perform MCMC</span>

nchains = 4                     <span class="comments">;number of markov chains to use</span>
checkiter = 100            <span class="comments">;check for convergence every 100 iterations</span>
iter = 0L

<span class="comments">;;;;;;;;;;;; get initial guesses for the MCMC</span>

<span class="comments">;; first use moment correction method to estimate regression</span>
<span class="comments">;; coefficients and intrinsic dispersion</span>

Xmat = [[replicate(1d, nx)], [x]]
denom = matrix_multiply(Xmat, Xmat, /atranspose)
Vcoef = denom
denom[1:*,1:*] = denom[1:*,1:*] - median(xvar, dim=1)

denom_diag = (denom[1:*,1:*])[diag]
denom_diag = denom_diag > 0.025 * (Vcoef[1:*,1:*])[diag]
denom[diag2[1:*]] = denom_diag
numer = y ## transpose(Xmat) - [0d, median(xycov, dim=1)]

choldc, denom, P, /double <span class="comments">;solve by cholesky decomposition</span>
coef = cholsol( denom, P, numer, /double )

alpha = coef[0]
beta = coef[1:*]

sigsqr = variance(y) - mean(yvar) - $
  beta ## (correlate(transpose(x), /covar) - median(xvar, dim=1)) ## transpose(beta)
sigsqr = sigsqr[0] > 0.05 * variance(y - alpha - beta ## x)

<span class="comments">; randomly disperse starting values for (alpha, beta) from a</span>
<span class="comments">; multivariate students-t distribution with 4 degrees of freedom</span>

mlinmix_posdef_invert, Vcoef
Vcoef = Vcoef * sigsqr * 4d

coef = mrandomn(seed, Vcoef, nchains)
chisqr = randomchi(seed, 4, nchains)

alphag = alpha + coef[*,0] * sqrt(4d / chisqr)
betag = dblarr(np, nchains)
for i = 0, nchains - 1 do betag[*,i] = beta + coef[i,1:*] * sqrt(4d / chisqr[i])

<span class="comments">;draw sigsqr from an Inverse scaled chi-square density</span>
sigsqrg = sigsqr * (nx / 2) / randomchi(seed, nx / 2, nchains)

<span class="comments">;; now get initial guesses for the mixture and prior parameters, do</span>
<span class="comments">;; this one chain at a time</span>

pig = dblarr(ngauss, nchains)
mug = dblarr(np, ngauss, nchains)
Tg = dblarr(np, np, ngauss, nchains)
mu0g = dblarr(np, nchains)
Ug = dblarr(np, np, nchains)
Wg = dblarr(np, np, nchains)

dist = dblarr(nx, ngauss)
Glabel = intarr(nx, nchains)

for i = 0, nchains - 1 do begin
    
                                <span class="comments">;randomly choose NGAUSS data points,</span>
                                <span class="comments">;set these to the group means</span>
    ind = lindgen(nx)
    unif = randomu(seed, nx)
    ind = (ind[sort(unif)])[0:ngauss-1]

    mug[*,*,i] = transpose(x[ind,*])
    
    if ngauss gt 1 then begin
                                <span class="comments">;get distance of data points to each</span>
                                <span class="comments">;centroid</span>
        for k = 0, ngauss - 1 do $
          dist[0,k] = total((x - mug[*,k,i] ## replicate(1d, nx))^2, 2)
        
        mindist = min(dist, Glabel0, dim=2) <span class="comments">;classify to closest centroid</span>
        
        Glabel0 = Glabel0 / nx

    endif else Glabel0 = intarr(nx)

    Glabel[0,i] = Glabel0

<span class="comments">;now get initial guesses for PI and T</span>

    for k = 0, ngauss - 1 do begin

        gk = where(Glabel0 eq k, nk)
        
        if nk gt np then begin

            pig[k,i] = float(nk) / nx
            Tg[*,*,k,i] = correlate(transpose(x[gk,*]), /covar)

        endif else begin

            pig[k,i] = (1d > nk) / nx
            Tg[*,*,k,i] = correlate(transpose(x), /covar)

        endelse

    endfor

    pig[*,i] = pig[*,i] / total(pig[*,i]) <span class="comments">;make sure Pi sums to unity</span>

<span class="comments">;now get initial guesses for prior parameters</span>

    mu0g[*,i] = ngauss eq 1 ? mug[*,0,i] : total(mug[*,*,i], 2) / ngauss
    Smat = correlate(transpose(x), /covar)
    Ug[*,*,i] = randomwish(seed, nx, Smat / nx)

    Wg[*,*,i] = randomwish(seed, nx, Smat / nx)

endfor

alpha = alphag
beta = betag
sigsqr = sigsqrg
pi = pig
mu = mug
T = Tg
mu0 = mu0g
U = Ug
W = Wg
                                <span class="comments">;get inverses of XYVAR</span>
xyvar_inv = xyvar
for i = 0, nx - 1 do begin
    
    xyvar_inv0 = reform(xyvar[i,*,*])
    mlinmix_posdef_invert, xyvar_inv0
    xyvar_inv[i,*,*] = xyvar_inv0
    
endfor
                                <span class="comments">;get staring values for eta</span>
eta = dblarr(nx, nchains)
for i = 0, nchains - 1 do eta[*,i] = y

nut = np <span class="comments">;degrees of freedom for the prior on T</span>
nuu = np <span class="comments">;degrees of freedom for the prior on U</span>

npar = 2 + np <span class="comments">;number of parameters to moniter convergence on</span>

convergence = 0
                                <span class="comments">;start Markov Chains</span>
if not silent then print, 'Simulating Markov Chains...'

ygibbs = y
                                <span class="comments">;define arrays now so we don't have to</span>
                                <span class="comments">;create them every MCMC iteration</span>
xi = dblarr(nx, np, nchains)
for i = 0, nchains - 1 do xi[*,*,i] = x
xstar = dblarr(nx, np)
mustar = dblarr(nx, np)
gamma = dblarr(nx, ngauss)
nk = fltarr(ngauss)
Tk_inv = dblarr(np, np, ngauss, nchains)
U_inv = dblarr(np, np, nchains)

                                <span class="comments">;get various matrix inverses before</span>
                                <span class="comments">;staring markov chain</span>
for i = 0, nchains - 1 do begin

    for k = 0, ngauss - 1 do begin
        
        Tk_inv0 = T[*,*,k,i]
        mlinmix_posdef_invert, Tk_inv0
        
        Tk_inv[*,*,k,i] = Tk_inv0
        
    endfor
    
    U_inv0 = U[*,*,i]
    mlinmix_posdef_invert, U_inv0
    U_inv[*,*,i] = U_inv0

endfor

repeat begin
 
    for i = 0, nchains - 1 do begin <span class="comments">;do markov chains one at-a-time</span>

        W_inv = W[*,*,i]
        mlinmix_posdef_invert, W_inv

<span class="comments">;do Gibbs sampler</span>
        if ncens gt 0 then begin
                                <span class="comments">;first get new values of censored y</span>
            for j = 0, ncens - 1 do begin
                
                next = 0
                repeat ygibbs[cens[j]] = eta[cens[j],i] + $
                  sqrt(yvar[cens[j]]) * randomn(seed) $
                  until ygibbs[cens[j]] le y[cens[j]]
                
            endfor
            
        endif
        
<span class="comments">;need to get new values of Xi and Eta for Gibbs sampler</span>
        
                                <span class="comments">;now draw Xi|mu,covar,x, do this for</span>
                                <span class="comments">;each covariate at a time</span>
        
        for j = 0, np - 1 do begin
            
            case j of
                
                0 : inactive = indgen(np - 1) + 1L
                np - 1 : inactive = indgen(np - 1)
                else : inactive = [indgen(j), indgen(np - j - 1) + j + 1]
                
            endcase
            
            xstar[*,j] = x[*,j]
            xstar[*,inactive] = x[*,inactive] - xi[*,inactive,i]
            
            zstar = [[ygibbs - eta[*,i]], [xstar]]
            
            zmu = total(xyvar_inv[*,*,j+1] * zstar, 2)
            
            for k = 0, ngauss - 1 do begin <span class="comments">;do one gaussian at-a-time</span>
                
                gk = where(Glabel[*,i] eq k, ngk)
                
                if ngk gt 0 then begin
                    
                    mustar[gk,j] = mu[j,k,i]
                    for l = 0, np - 2 do mustar[gk,inactive[l]] = $
                      mu[inactive[l],k,i] - xi[gk,inactive[l],i]
                    
                    mmu = Tk_inv[*,j,k,i] ## mustar[gk,*]
                    
                    etamu = eta[gk,i] - alpha[i] - beta[inactive,i] ## xi[gk,inactive,i]
                    
                    xihvar = 1d / (xyvar_inv[gk,j+1,j+1] + Tk_inv[j,j,k,i] + $
                                   beta[j,i]^2 / sigsqr[i])
                    
                    xihat = xihvar * (zmu[gk] + mmu + beta[j,i] * etamu / (sigsqr[i]))
                    
                    xi[gk,j,i] = xihat + sqrt(xihvar) * randomn(seed, nx)
                    
                endif
                
            endfor
            
        endfor
                                <span class="comments">;now draw Eta|Xi,alpha,beta,sigsqr,y</span>
        zstar = [[ygibbs], [x - xi[*,*,i]]]
        
        zmu = total(xyvar_inv[*,*,0] * zstar, 2)
        
        ximu = (alpha[i] + beta[*,i] ## xi[*,*,i]) / sigsqr[i]
        
        etahvar = 1d / (xyvar_inv[*,0,0] + 1d / sigsqr[i])
        etahat = etahvar * (zmu + ximu)
        
        eta[*,i] = etahat + sqrt(etahvar) * randomn(seed, nx)
        
                                <span class="comments">;now draw new class labels</span>
        if ngauss eq 1 then Glabel[*,i] = 0 else begin
                                <span class="comments">;get unnormalized probability that</span>
                                <span class="comments">;source i came from Gaussian k, given</span>
                                <span class="comments">;xi[i]</span>
            for k = 0, ngauss - 1 do begin
                
                xicent = xi[*,*,i] - mu[*,k,i] ## replicate(1, nx)
                gamma[0,k] = $
                  pi[k,i] / ((2d*!pi)^(np/2d) * determ(T[*,*,k,i], /double)) * $
                  exp(-0.5 * total(xicent * (Tk_inv[*,*,k,i] ## xicent), 2))
                
            endfor
            
            norm = total(gamma, 2)
            
            for j = 0, nx - 1 do begin
                
                gamma0 = reform(gamma[j,*]) / norm[j] <span class="comments">;normalized probability that the i-th </span>
                                                      <span class="comments">;data point is from the k-th Gaussian, </span>
                                                      <span class="comments">;given the observed data point </span>
                Gjk = multinom(1, gamma0, seed=seed)
                
                Glabel[j,i] = where(Gjk eq 1)
                
            endfor
            
        endelse

<span class="comments">;; now draw new values of alpha, beta, and sigsqr</span>
        
                                <span class="comments">;first do alpha,beta|Xi,Eta,sigsqr</span>
        
        Xmat[*,1:*] = xi[*,*,i]
        
        hatmat = matrix_multiply(Xmat, Xmat, /atranspose)
        Vcoef = hatmat
        
        choldc, hatmat, P, /double <span class="comments">;solve by cholesky decomposition</span>
        coefhat = cholsol( hatmat, P, eta[*,i] ## transpose(Xmat), /double )
        
        mlinmix_posdef_invert, Vcoef
        Vcoef = Vcoef * sigsqr[i]
        
        coef = coefhat + mrandomn(seed, Vcoef)
        
        alpha[i] = coef[0]
        beta[*,i] = coef[1:*]
        
                                <span class="comments">;now do sigsqr|xi,eta,alpha,beta, </span>
                                <span class="comments">;draw sigsqr from a scaled</span>
                                <span class="comments">;Inverse-chi-square density</span>
        resid = eta[*,i] - alpha[i] - beta[*,i] ## xi[*,*,i]
        ssqr = total( resid^2 ) / (nx - 2d)
        
        sigsqr[i] = ssqr * (nx - 2d) / randomchi(seed, nx - 2)

<span class="comments">;; now do mixture model parameters, psi = (pi,mu,tausqr)</span>

        for k = 0, ngauss - 1 do begin 
            
            gk = where(Glabel[*,i] eq k, ngk)
            nk[k] = ngk
           
            if ngk gt 0 then begin
                                <span class="comments">;get mu|Xi,G,tausqr,mu0,U</span>
                
                muvar = U_inv[*,*,i] + ngk * Tk_inv[*,*,k,i]
                mlinmix_posdef_invert, muvar
                
                xibar = total(xi[gk,*,i], 1) / ngk
                
                muhat = (mu0[*,i] ## U_inv[*,*,i] + $
                         ngk * (xibar ## Tk_inv[*,*,k,i])) ## muvar
                
                mu[*,k,i] = muhat + mrandomn(seed, muvar)
                
            endif else mu[*,k,i] = mu0[*,i] + mrandomn(seed, U[*,*,i])

                                <span class="comments">;get T|Xi,G,mu,W,nut</span>
            nuk = ngk + nut

            if ngk gt 0 then begin
                
                xicent = xi[gk,*,i] - mu[*,k,i] ## replicate(1d, ngk)
                
                Smat = W[*,*,i] + xicent ## transpose(xicent)
                
                Smat_inv = Smat
                mlinmix_posdef_invert, Smat_inv

            endif else begin

                Smat = W
                Smat_inv = W_inv

            endelse

            Tmat = randomwish(seed, nuk, Smat_inv)
            
            Tk_inv[*,*,k,i] = Tmat
            mlinmix_posdef_invert, Tmat
            T[*,*,k,i] = Tmat
            
        endfor
                                <span class="comments">;get pi|G</span>
        if ngauss eq 1 then pi[*,i] = 1d else $
          pi[*,i] = randomdir(seed, nk + 1)
        
<span class="comments">;; now, finally update the prior parameters    </span>
        
                                <span class="comments">;first update mean of gaussian</span>
                                <span class="comments">;centroids</span>
        mu0[*,i] = ngauss eq 1 ? mu[*,0,i] + mrandomn(seed, U[*,*,i]) : $
          total(mu[*,*,i], 2) / ngauss + mrandomn(seed, U[*,*,i] / ngauss)

                                <span class="comments">;update centroid covariance matrix, U</span>
        nu = ngauss + nuu
        
        mucent = ngauss eq 1 ? transpose(mu[*,0,i] - mu0[*,i]) : $
          transpose(mu[*,*,i]) - mu0[*,i] ## replicate(1d, ngauss)
        
        Uhat = W[*,*,i] + mucent ## transpose(mucent)
        
        mlinmix_posdef_invert, Uhat
        Umat = randomwish(seed, nu, Uhat)
        
        U_inv[*,*,i] = Umat
        mlinmix_posdef_invert, Umat
        U[*,*,i] = Umat
 
                                <span class="comments">;update the common scale matrix, W</span>
        nuw = (ngauss + 2) * np + 1
        What = ngauss eq 1 ? U_inv[*,*,i] + Tk_inv[*,*,0,i] : $
          U_inv[*,*,i] + total(Tk_inv[*,*,*,i], 3)
        
        mlinmix_posdef_invert, What
        
        W[*,*,i] = randomwish(seed, nuw, What)

    endfor
                                <span class="comments">;save Markov Chains</span>
    if iter eq 0 then begin
        
        alphag = alpha
        betag = beta[*]
        sigsqrg = sigsqr
        
        pig = pi[*]
        mug = mu[*]
        Tg = T[*]
        
        mu0g = mu0[*]
        Ug = U[*]
        Wg = W[*]
    
    endif else begin
        
        alphag = [alphag, alpha]
        betag = [betag, beta[*]]
        sigsqrg = [sigsqrg, sigsqr]

        pig = [pig, pi[*]]
        mug = [mug, mu[*]]
        Tg = [Tg, T[*]]

        mu0g = [mu0g, mu0[*]]
        Ug = [Ug, U[*]]
        Wg = [Wg, W[*]]

    endelse
    
    iter = iter + 1L
    
<span class="comments">;check for convergence</span>
    
    if iter ge 4 then begin
        
        Bvar = dblarr(npar)     <span class="comments">;between-chain variance</span>
        Wvar = dblarr(npar)     <span class="comments">;within-chain variance</span>
        
        ndraw = n_elements(alphag) / nchains
        
        psi = dblarr(npar, nchains, ndraw)
        psi[0,*,*] = reform(alphag, nchains, ndraw)
        psi[1:np,*,*] = reform(betag, np, nchains, ndraw)
        psi[np+1,*,*] = alog(reform(sigsqrg, nchains, ndraw))
        
        psi = psi[*,*,(ndraw+1)/2:*]
        ndraw = ndraw / 2
                                <span class="comments">;calculate between- and within-sequence</span>
                                <span class="comments">;                  variances</span>
        for j = 0, npar - 1 do begin
            
            psibarj = total( psi[j,*,*], 3 ) / ndraw
            psibar = mean(psibarj)
            
            sjsqr = 0d
            for i = 0, nchains - 1 do $
              sjsqr = sjsqr + total( (psi[j, i, *] - psibarj[i])^2 ) / (ndraw - 1.0)
            
            Bvar[j] = ndraw / (nchains - 1.0) * total( (psibarj - psibar)^2 )
            Wvar[j] = sjsqr / nchains
            
        endfor
        
        varplus = (1.0 - 1d / ndraw) * Wvar + Bvar / ndraw
        Rhat = sqrt( varplus / Wvar ) <span class="comments">;potential variance scale reduction factor</span>
        
    endif
    
    if iter eq checkiter then begin 
<span class="comments">;maximum iterations reached, now assess convergence</span>

        if (total( (Rhat le 1.1) ) eq npar and iter ge miniter) or $
          iter ge maxiter then convergence = 1 $
        else begin
            
            if not silent then begin
                print, 'Iteration: ', iter
                print, 'Rhat Values (ALPHA, BETA, SIGSQR) : '
                print, Rhat
            endif
            
            checkiter = checkiter + 100L
            
        endelse

    endif
    
endrep until convergence

ndraw = n_elements(alphag) / nchains

alphag = reform(alphag, nchains, ndraw)
betag = reform(betag, np, nchains, ndraw)
sigsqrg = reform(sigsqrg, nchains, ndraw)

pig = reform(pig, ngauss, nchains, ndraw)
mug = reform(mug, np, ngauss, nchains, ndraw)
Tg = reform(Tg, np, np, ngauss, nchains, ndraw)

mu0g = reform(mu0g, np, nchains, ndraw)
Ug = reform(Ug, np, np, nchains, ndraw)
Wg = reform(Wg, np, np, nchains, ndraw)

<span class="comments">;only keep second half of markov chains</span>
alphag = alphag[*,(ndraw+1)/2:*]
betag = betag[*,*,(ndraw+1)/2:*]
sigsqrg = sigsqrg[*,(ndraw+1)/2:*]
pig = pig[*,*,(ndraw+1)/2:*]
mug = mug[*,*,*,(ndraw+1)/2:*]
Tg = Tg[*,*,*,*,(ndraw+1)/2:*]
mu0g = mu0g[*,*,(ndraw+1)/2:*]
Ug = Ug[*,*,*,(ndraw+1)/2:*]
Wg = Wg[*,*,*,(ndraw+1)/2:*]

if not silent then begin
    print, 'Iteration: ', iter
    print, 'Rhat Values (ALPHA, BETA, SIGSQR) : ', Rhat
endif

<span class="comments">;save posterior draws in a structure</span>
ndraw = ndraw / 2


if ngauss gt 1 then $
  post = {alpha:0d, beta:dblarr(np), sigsqr:0d, pi:dblarr(ngauss), mu:dblarr(np,ngauss), $
          T:dblarr(np,np,ngauss), mu0:dblarr(np), U:dblarr(np,np), W:dblarr(np,np), $
          ximean:dblarr(np), xivar:dblarr(np,np), xicorr:dblarr(np,np), corr:dblarr(np), $
          pcorr:dblarr(np)} $
else $
  post = {alpha:0d, beta:dblarr(np), sigsqr:0d, pi:0d, mu:dblarr(np), $
          T:dblarr(np,np), mu0:dblarr(np), U:dblarr(np,np), W:dblarr(np,np), $
          ximean:dblarr(np), xivar:dblarr(np,np), xicorr:dblarr(np,np), corr:dblarr(np), $
          pcorr:dblarr(np)}

post = replicate(post, ndraw * nchains)

post.alpha = alphag[*]
post.beta = reform(betag, np, ndraw * nchains)
post.sigsqr = sigsqrg[*]

if ngauss gt 1 then begin

    post.pi = reform(pig, ngauss, ndraw * nchains)
    post.mu = reform(mug, np, ngauss, ndraw * nchains)
    post.T = reform(Tg, np, np, ngauss, ndraw * nchains)

endif else begin

    post.pi = reform(pig, ndraw * nchains)
    post.mu = reform(mug, np, ndraw * nchains)
    post.T = reform(Tg, np, np, ndraw * nchains)

endelse

post.mu0 = reform(mu0g, np, ndraw * nchains)
post.U = reform(Ug, np, np, ndraw * nchains)
post.W = reform(Wg, np, np, ndraw * nchains)

<span class="comments">;get posterior draws of moments of distribution</span>

if not silent then print, 'Getting Posterior Draws for Various Moments...'

corrmat = dblarr(np+1,np+1)

for i = 0, ndraw * nchains - 1 do begin
                                <span class="comments">;average value of Xi</span>
    post[i].ximean = ngauss gt 1 ? post[i].pi ## post[i].mu : post[i].mu

    if ngauss eq 1 then post[i].xivar = post[i].T else begin

        for k = 0, ngauss - 1 do post[i].xivar = post[i].xivar + $
          post[i].pi[k] * (post[i].T[*,*,k] + transpose(post[i].mu[*,k]) ## post[i].mu[*,k])
                                <span class="comments">;covariance matrix of Xi</span>
        post[i].xivar = post[i].xivar - transpose(post[i].ximean) ## post[i].ximean

    endelse       
    
    xivar = post[i].xivar
    
                                <span class="comments">;variance in Eta</span>
    etavar = post[i].beta ## post[i].xivar ## transpose(post[i].beta) + post[i].sigsqr
                                <span class="comments">;correlation coefficients between Eta</span>
                                <span class="comments">;and Xi</span>
    post[i].corr = post[i].beta ## post[i].xivar / $
      sqrt( etavar[0] * post[i].xivar[diag] )
                                <span class="comments">;correlation matrix of the covariates</span>
    post[i].xicorr = xivar * ( transpose(1d / sqrt(xivar[diag])) ## (1d / sqrt(xivar[diag])) )
                                <span class="comments">;now get partial correlations, need</span>
                                <span class="comments">;full correlation matrix first</span>
    corrmat[0,0] = 1d
    corrmat[1:*,0] = post[i].corr
    corrmat[0,1:*] = post[i].corr
    corrmat[1:*,1:*] = post[i].xicorr

    mlinmix_posdef_invert, corrmat

    post[i].pcorr = -1d * corrmat[1:*,0] / sqrt(corrmat[0,0] * corrmat[diag2[1:*]])

endfor

return
end
</code>
    </div>
  </body>
</html>